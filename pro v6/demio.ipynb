{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6351c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Extract keypoints from frame\n",
    "def extract_landmarks(results):\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 3)\n",
    "    left = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    right = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([pose, left, right])  # total 225 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155c57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_from_video(video_path, max_frames=117):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    sequence = []\n",
    "\n",
    "    with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = holistic.process(image)\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            keypoints = extract_landmarks(results)\n",
    "            sequence.append(keypoints)\n",
    "\n",
    "            if len(sequence) >= max_frames:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda666c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3557, 117, 225)\n",
      "y shape: (3557,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_FRAMES = 117  # maximum frame length to extract\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X, y = [], []\n",
    "labels = []\n",
    "\n",
    "#DATASET_PATH = 'D:/code/Mini/Final data'\n",
    "OUTPUT_PATH = 'D:/code/Mini/pro v6/extracted_sequences'\n",
    "for word in os.listdir(OUTPUT_PATH):\n",
    "    word_path = os.path.join(OUTPUT_PATH, word)\n",
    "    for file in os.listdir(word_path):\n",
    "        sequence = np.load(os.path.join(word_path, file))\n",
    "\n",
    "        # Pad/truncate\n",
    "        if sequence.shape[0] < MAX_FRAMES:\n",
    "            pad_len = MAX_FRAMES - sequence.shape[0]\n",
    "            padding = np.zeros((pad_len, 225))\n",
    "            sequence = np.vstack((sequence, padding))\n",
    "        else:\n",
    "            sequence = sequence[:MAX_FRAMES]\n",
    "\n",
    "        X.append(sequence)\n",
    "        y.append(word)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X shape:\", X.shape)  # should be (num_samples, 117, 225)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee866552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['1. loud' '10. Mean' '11. rich' '12. poor' '13. thick' '17. flat'\n",
      " '18. City' '18. curved' '19. House' '19. male' '2. quiet'\n",
      " '20. Street or Road' '20. female' '21. Train Station' '22. Restaurant'\n",
      " '23. Court' '23. high' '24. School' '24. low' '25. Office' '25. soft'\n",
      " '26. University' '26. hard' '27. Park' '27. deep' '28. shallow'\n",
      " '29. clean' '3. happy' '30. dirty' '31. strong' '32. weak' '33. dead'\n",
      " '34. alive' '35. heavy' '36. light' '37. Hat' '38. Dress' '39. Key'\n",
      " '39. Suit' '39. famous' '4. sad' '40. I' '40. Paint' '40. Skirt'\n",
      " '41. Letter' '41. Shirt' '41. you' '42. Paper' '42. T-Shirt' '42. he'\n",
      " '43. Lock' '43. Pant' '43. she' '44. Shoes' '44. Telephone' '44. it'\n",
      " '45. Bag' '45. Pocket' '45. we' '46. Box' '46. Clothing'\n",
      " '46. you (plural)' '47. Gift' '47. they' '48. Card' '48. Hello'\n",
      " '49. How are you' '49. Ring' '5. Beautiful' '50. Alright' '50. Tool'\n",
      " '51. Good Morning' '52. Good afternoon' '58. Son' '59. Daughter'\n",
      " '6. Ugly' '60. Mother' '61. Father' '62. Parent' '63. Baby' '64. Man'\n",
      " '65. Woman' '66. Brother' '67. Sister' '7. Deaf' '78. long' '79. short'\n",
      " '8. Blind' '80. tall' '81. wide' '82. narrow' '83. big large'\n",
      " '84. Teacher' '84. small little' '85. Student' '85. slow' '86. Lawyer'\n",
      " '86. fast' '87. Doctor' '88. Patient' '89. Waiter' '90. Secretary'\n",
      " '91. Priest']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Classes:\", label_encoder.classes_)  # list of your 50 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a2e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stavya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">362,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">103</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,695</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m225\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m362,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m103\u001b[0m)            │         \u001b[38;5;34m6,695\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">541,799</span> (2.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m541,799\u001b[0m (2.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">541,799</span> (2.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m541,799\u001b[0m (2.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(117, 225)))  # mask padded zeros\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bddf1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - accuracy: 0.0078 - loss: 4.6397 - val_accuracy: 0.0239 - val_loss: 4.5047\n",
      "Epoch 2/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - accuracy: 0.0252 - loss: 4.4579 - val_accuracy: 0.0435 - val_loss: 4.1909\n",
      "Epoch 3/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.0310 - loss: 4.2093 - val_accuracy: 0.0618 - val_loss: 3.9980\n",
      "Epoch 4/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - accuracy: 0.0473 - loss: 4.0388 - val_accuracy: 0.0674 - val_loss: 3.8392\n",
      "Epoch 5/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.0614 - loss: 3.8660 - val_accuracy: 0.0787 - val_loss: 3.6719\n",
      "Epoch 6/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.0816 - loss: 3.6993 - val_accuracy: 0.0843 - val_loss: 3.5833\n",
      "Epoch 7/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 89ms/step - accuracy: 0.0897 - loss: 3.5841 - val_accuracy: 0.1138 - val_loss: 3.4832\n",
      "Epoch 8/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.1145 - loss: 3.5073 - val_accuracy: 0.1615 - val_loss: 3.2085\n",
      "Epoch 9/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 285ms/step - accuracy: 0.1381 - loss: 3.2273 - val_accuracy: 0.2107 - val_loss: 3.0495\n",
      "Epoch 10/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 292ms/step - accuracy: 0.1352 - loss: 3.2039 - val_accuracy: 0.1840 - val_loss: 3.0289\n",
      "Epoch 11/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 300ms/step - accuracy: 0.1675 - loss: 3.0675 - val_accuracy: 0.2051 - val_loss: 2.9358\n",
      "Epoch 12/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 286ms/step - accuracy: 0.1985 - loss: 2.9295 - val_accuracy: 0.3020 - val_loss: 2.6751\n",
      "Epoch 13/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 305ms/step - accuracy: 0.2334 - loss: 2.7852 - val_accuracy: 0.3216 - val_loss: 2.5438\n",
      "Epoch 14/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 280ms/step - accuracy: 0.2294 - loss: 2.7056 - val_accuracy: 0.3118 - val_loss: 2.5162\n",
      "Epoch 15/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 293ms/step - accuracy: 0.2626 - loss: 2.6317 - val_accuracy: 0.3610 - val_loss: 2.3702\n",
      "Epoch 16/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 263ms/step - accuracy: 0.3099 - loss: 2.4015 - val_accuracy: 0.3919 - val_loss: 2.2872\n",
      "Epoch 17/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 278ms/step - accuracy: 0.3259 - loss: 2.3659 - val_accuracy: 0.3736 - val_loss: 2.3589\n",
      "Epoch 18/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 247ms/step - accuracy: 0.3229 - loss: 2.3810 - val_accuracy: 0.4270 - val_loss: 2.0556\n",
      "Epoch 19/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.3874 - loss: 2.1517 - val_accuracy: 0.4635 - val_loss: 1.9115\n",
      "Epoch 20/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4145 - loss: 2.0559 - val_accuracy: 0.4691 - val_loss: 1.9035\n",
      "Epoch 21/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 303ms/step - accuracy: 0.3950 - loss: 2.0822 - val_accuracy: 0.4944 - val_loss: 1.7919\n",
      "Epoch 22/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 289ms/step - accuracy: 0.4644 - loss: 1.8674 - val_accuracy: 0.4719 - val_loss: 1.8213\n",
      "Epoch 23/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 163ms/step - accuracy: 0.4479 - loss: 1.8333 - val_accuracy: 0.5154 - val_loss: 1.6548\n",
      "Epoch 24/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.4806 - loss: 1.7481 - val_accuracy: 0.5351 - val_loss: 1.5830\n",
      "Epoch 25/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.4923 - loss: 1.7242 - val_accuracy: 0.5688 - val_loss: 1.4987\n",
      "Epoch 26/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.5067 - loss: 1.6487 - val_accuracy: 0.5716 - val_loss: 1.4838\n",
      "Epoch 27/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.5616 - loss: 1.4848 - val_accuracy: 0.5829 - val_loss: 1.4218\n",
      "Epoch 28/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.5693 - loss: 1.4135 - val_accuracy: 0.6348 - val_loss: 1.2705\n",
      "Epoch 29/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 102ms/step - accuracy: 0.5817 - loss: 1.3458 - val_accuracy: 0.6208 - val_loss: 1.3100\n",
      "Epoch 30/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.5970 - loss: 1.3388 - val_accuracy: 0.6362 - val_loss: 1.2277\n",
      "Epoch 31/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.6106 - loss: 1.2709 - val_accuracy: 0.6517 - val_loss: 1.1988\n",
      "Epoch 32/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.6076 - loss: 1.2953 - val_accuracy: 0.6671 - val_loss: 1.1111\n",
      "Epoch 33/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.6667 - loss: 1.1173 - val_accuracy: 0.6812 - val_loss: 1.0904\n",
      "Epoch 34/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.6542 - loss: 1.1574 - val_accuracy: 0.6924 - val_loss: 1.0329\n",
      "Epoch 35/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.6630 - loss: 1.1229 - val_accuracy: 0.7135 - val_loss: 0.9785\n",
      "Epoch 36/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.6980 - loss: 0.9775 - val_accuracy: 0.7360 - val_loss: 0.9426\n",
      "Epoch 37/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.6784 - loss: 1.0207 - val_accuracy: 0.7149 - val_loss: 0.9401\n",
      "Epoch 38/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.7269 - loss: 0.9381 - val_accuracy: 0.7556 - val_loss: 0.8701\n",
      "Epoch 39/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.7049 - loss: 0.9496 - val_accuracy: 0.6938 - val_loss: 1.0225\n",
      "Epoch 40/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.7004 - loss: 0.9953 - val_accuracy: 0.7360 - val_loss: 0.8795\n",
      "Epoch 41/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.7214 - loss: 0.9010 - val_accuracy: 0.7781 - val_loss: 0.7586\n",
      "Epoch 42/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.7605 - loss: 0.7787 - val_accuracy: 0.7486 - val_loss: 0.8144\n",
      "Epoch 43/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.7432 - loss: 0.8394 - val_accuracy: 0.7949 - val_loss: 0.7061\n",
      "Epoch 44/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.7663 - loss: 0.7486 - val_accuracy: 0.7823 - val_loss: 0.7565\n",
      "Epoch 45/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.7798 - loss: 0.6992 - val_accuracy: 0.7725 - val_loss: 0.7425\n",
      "Epoch 46/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.7645 - loss: 0.7430 - val_accuracy: 0.7978 - val_loss: 0.7048\n",
      "Epoch 47/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.7933 - loss: 0.6761 - val_accuracy: 0.8034 - val_loss: 0.6891\n",
      "Epoch 48/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.7867 - loss: 0.6896 - val_accuracy: 0.7683 - val_loss: 0.7548\n",
      "Epoch 49/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.7916 - loss: 0.6389 - val_accuracy: 0.8062 - val_loss: 0.6365\n",
      "Epoch 50/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 107ms/step - accuracy: 0.8040 - loss: 0.6359 - val_accuracy: 0.8048 - val_loss: 0.6660\n",
      "Epoch 51/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.8076 - loss: 0.6650 - val_accuracy: 0.8399 - val_loss: 0.5898\n",
      "Epoch 52/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.8332 - loss: 0.5514 - val_accuracy: 0.8343 - val_loss: 0.5908\n",
      "Epoch 53/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 262ms/step - accuracy: 0.8285 - loss: 0.5287 - val_accuracy: 0.8399 - val_loss: 0.5715\n",
      "Epoch 54/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 271ms/step - accuracy: 0.8348 - loss: 0.5152 - val_accuracy: 0.8455 - val_loss: 0.5339\n",
      "Epoch 55/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 270ms/step - accuracy: 0.8731 - loss: 0.4377 - val_accuracy: 0.8497 - val_loss: 0.5467\n",
      "Epoch 56/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 137ms/step - accuracy: 0.8564 - loss: 0.4523 - val_accuracy: 0.8258 - val_loss: 0.5611\n",
      "Epoch 57/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.8118 - loss: 0.5543 - val_accuracy: 0.8174 - val_loss: 0.5779\n",
      "Epoch 58/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.8362 - loss: 0.5421 - val_accuracy: 0.8581 - val_loss: 0.5148\n",
      "Epoch 59/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.8653 - loss: 0.4244 - val_accuracy: 0.8708 - val_loss: 0.4627\n",
      "Epoch 60/60\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.8732 - loss: 0.4075 - val_accuracy: 0.8876 - val_loss: 0.4285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=60,  # feel free to tune\n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43040a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"isl_bilstm_model_v3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"label_encoder_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
